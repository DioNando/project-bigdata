# Configuration Docker Compose pour Airflow et services complémentaires
x-airflow-common: &airflow-common
  # image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.10.3}
  image: ${AIRFLOW_IMAGE_NAME:-mahefafernando/airflow:2.0.0}
  environment: &airflow-common-env
    # Configuration du backend pour Airflow
    AIRFLOW__CORE__EXECUTOR: CeleryExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow
    AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0

    # Configuration de sécurité et gestion des DAGs
    AIRFLOW__CORE__FERNET_KEY: ""
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "true"

    # Authentification via API
    AIRFLOW__API__AUTH_BACKENDS: "airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session"

    # Paramètres pour les notifications par e-mail
    AIRFLOW__SMTP__SMTP_HOST: maildev
    AIRFLOW__SMTP__SMTP_PORT: 1025
    AIRFLOW__SMTP__SMTP_USER:
    AIRFLOW__SMTP__SMTP_PASSWORD:
    AIRFLOW__SMTP__SMTP_MAIL_FROM: airflow@example.com
    AIRFLOW__SMTP__SMTP_STARTTLS: 'False'
    AIRFLOW__SMTP__SMTP_SSL: 'False'

    # Configuration pour le producer
    KAFKA_TOPIC: ${KAFKA_TOPIC}  # Topic Kafka sur lequel envoyer les messages
    KAFKA_SERVER: ${KAFKA_SERVER}  # Serveur Kafka

    REDDIT_ACCESS_TOKEN: ${REDDIT_ACCESS_TOKEN}


  volumes:
    # Montage des volumes pour DAGs et logs
    - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags
    - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs
  user: "${AIRFLOW_UID:-50000}:0"
  depends_on: &airflow-common-depends-on
    redis:
      condition: service_healthy
    postgres:
      condition: service_healthy

services:
  # Base de données PostgreSQL pour Airflow
  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-data-volume:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 10s
      retries: 5
      start_period: 5s
    restart: unless-stopped

  # Redis comme message broker pour CeleryExecutor
  redis:
    image: redis:7.2-bookworm
    expose:
      - 6379
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 30s
      retries: 50
      start_period: 30s
    restart: unless-stopped

  # Service Zookeeper (nécessaire pour Kafka)
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.1
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    restart: unless-stopped

  # Kafka pour la gestion des flux de données
  kafka:
    image: confluentinc/cp-kafka:7.4.1
    depends_on:
      - zookeeper    
    restart: unless-stopped
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1

  # Interface web pour visualiser les topics et les messages dans Kafka
  kafdrop:
    image: obsidiandynamics/kafdrop:latest
    depends_on:
      - kafka
    ports:
      - "9000:9000"
    environment:
      KAFKA_BROKERCONNECT: kafka:9092

  # Interface web Airflow
  airflow-webserver:
    <<: *airflow-common
    command: webserver
    ports:
      - "8080:8080" # Port local pour accéder à l'interface web
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  # Scheduler pour planifier les tâches
  airflow-scheduler:
    <<: *airflow-common
    command: scheduler
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8974/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  # Worker Celery pour exécuter les tâches
  airflow-worker:
    <<: *airflow-common
    command: celery worker
    healthcheck:
      test:
        - "CMD-SHELL"
        - 'celery --app airflow.providers.celery.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}" || celery --app airflow.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}"'
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    environment:
      <<: *airflow-common-env
      DUMB_INIT_SETSID: "0" # Evite les problèmes liés à dumb-init
    restart: unless-stopped
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  # Service d'initialisation Airflow
  airflow-init:
    <<: *airflow-common
    entrypoint: /bin/bash
    command:
      - -c
      - |
        mkdir -p /sources/logs /sources/dags
        chown -R "${AIRFLOW_UID}:0" /sources/{logs,dags}
        exec /entrypoint airflow version
    environment:
      <<: *airflow-common-env
      _AIRFLOW_DB_MIGRATE: "true"
      _AIRFLOW_WWW_USER_CREATE: "true"
      _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}
      _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}
    user: "0:0"
    volumes:
      - ${AIRFLOW_PROJ_DIR:-.}:/sources

  # MongoDB pour stocker les données
  mongodb:
    image: mongo:6.0
    restart: unless-stopped
    environment:
      MONGO_INITDB_ROOT_USERNAME: root
      MONGO_INITDB_ROOT_PASSWORD: example
    ports:
      - "27017:27017"
    volumes:
      - mongodb-data-volume:/data/db
      - ./init-mongo:/docker-entrypoint-initdb.d

  # Interface web pour MongoDB
  mongo-express:
    image: mongo-express:1.0.0-alpha.4
    restart: unless-stopped
    environment:
      ME_CONFIG_MONGODB_ADMINUSERNAME: root
      ME_CONFIG_MONGODB_ADMINPASSWORD: example
      ME_CONFIG_MONGODB_SERVER: mongodb
    ports:
      - "8081:8081"

  # # MailDev pour tester les e-mails envoyés par Airflow
  # maildev:
  #   image: maildev/maildev
  #   ports:
  #     - "1080:1080" # Interface web MailDev
  #     - "1025:1025" # Port SMTP pour capturer les e-mails

  # # Consumer, service qui consomme les messages de Kafka et les enregistre dans MongoDB
  # consumer:
  #   build:
  #     context: ./consumer  # Chemin du Dockerfile du consumer
  #   restart: unless-stopped  # Redémarre le service si il échoue
  #   environment:
  #     KAFKA_TOPIC: ${KAFKA_TOPIC}  # Topic Kafka à consommer
  #     KAFKA_SERVER: ${KAFKA_SERVER}  # Serveur Kafka
  #     MONGO_URI: ${MONGO_URI}  # URI de connexion à MongoDB
  #     MONGO_DB: ${MONGO_DB}  # Nom de la base de données MongoDB
  #     MONGO_COLLECTION: ${MONGO_COLLECTION}  # Nom de la collection MongoDB
  #   depends_on:
  #     - kafka  # Le consumer dépend de Kafka pour recevoir les messages
  #     - mongodb  # Le consumer dépend de MongoDB pour stocker les données

  # # Streamlit, service de visualisation des données météo avec une interface web
  # streamlit:
  #   build: ./streamlit  # Chemin du Dockerfile de Streamlit
  #   environment:
  #     MONGO_URI: ${MONGO_URI}  # URI de connexion à MongoDB
  #     MONGO_DB: ${MONGO_DB}  # Nom de la base de données MongoDB
  #     MONGO_COLLECTION: ${MONGO_COLLECTION}  # Nom de la collection MongoDB
  #     FETCH_INTERVAL: ${FETCH_INTERVAL}  # Intervalle de temps pour récupérer les données (en secondes)
  #   ports:
  #     - "8501:8501"  # Expose l'application Streamlit sur le port 8501
  #   depends_on:
  #     - mongodb  # Streamlit dépend de MongoDB pour accéder aux données

volumes:
  postgres-data-volume:
  mongodb-data-volume:
